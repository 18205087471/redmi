Keepalived(VRRP协议)---->VIP

LVS+Keepalived实现高可用
可以解决单点故障,自动报警
user
		nginx+keepalived  100   vip4.80
VIP   
		nginx+keepalived  50     vip4.80
路由器功能
1.自动配置VIP
2.自动配置LVS健康检查
iptables防火墙
iptables -F 清空drop规则
ip addr show
VIP地址

1. yum install -y keepalived
2. vim /etc/keepalived/keepalived.conf
web1:
global_defs {
  notification_email {
    admin@tarena.com.cn                //设置报警收件人邮箱
  }
	... ... ...
	router_id  web1                        //设置路由ID号
  }
vrrp_instance VI_1 {
	... ... ...
virtual_ipaddress {                   //谁是主服务器谁获得该VIP
	192.168.4.80
 } 
 }

web2同样修改
router_id web2         //名称
state BACKUP         //备服务器为BACKUP
priority 50            //权重

systemctl restart keepalived
iptables -F


校验值md5sum
if [   $a  ==  "md5值"  ]

Keepalived高可用服务器
部署Keepalived实现LVS-DR模式调度器的高可用
global_defs {
		...................
router_id Lvs1             //路由ID号
		...................
virtual_ipaddress {                   //配置VIP
		192.168.4.15
 }
}
virtual_server 192.168.4.15 80 {           //设置ipvsadm的VIP规则
  delay_loop 6
  lb_algo wrr                          //设置LVS调度算法为WRR
  lb_kind DR                               //设置LVS的模式为DR
  #persistence_timeout 50
#注意这样的作用是保持连接，开启后，客户端在一定时间内始终访问相同服务器
protocol TCP
real_server 192.168.4.100 80 {         //设置后端web服务器真实IP
         weight 1                             //设置权重为1
         TCP_CHECK {                      //对后台real_server做健康检查
         connect_timeout 3
         nb_get_retry 3
         delay_before_retry 3
    }
  }
复制一份,然后修改
global_defs {
		.................
router_id Lvs2             //路由ID号
		...................
real_server 192.168.4.200 80 {       //设置后端web服务器真实IP
          weight 2                          //设置权重为2
          TCP_CHECK {			//
          connect_timeout 3
时间    nb_get_retry 3
          delay_before_retry 3
    }
  }
}
web2:
上述操作一样,
vrrp_instance VI_1 {
  state BACKUP                             //从服务器为BACKUP（实验需要修改）
  interface eth0                        //定义网络接口
  virtual_router_id 50                    //主辅VRID号必须一致
  priority 50                             //服务器优先级（实验需要修改）
  advert_int 1

[root@proxy2 ~]# systemctl start keepalived
[root@proxy2 ~]# ipvsadm -Ln                 #查看LVS规则
[root@proxy2 ~]# ip  a   s                    #查看VIP设置
客户端测试
优先级




配置HAProxy负载平衡集群
HAProxy	Nginx     原理一样
将前面实验VIP、LVS等实验的内容清理干净！！！！！！
删除所有设备的VIP，清空所有LVS设置，关闭keepalived！！！
关闭多余网卡与VIP

部署HAProxy服务器
配置网络，安装软件
[root@proxy ~]# yum -y install haproxy
修改配置文件
vim /etc/haproxy/haproxy.cfg
global全局设置
pidfile /var/run/haproxy.pid 	###haproxy的pid存放路径
 maxconn 4000     #最大连接数，默认4000
            	.............
daemon       	#创建1个进程进入deamon模式运行
defaults 默认设置
  mode http 	#默认的模式mode { tcp|http|health } log global   #采用全局定义的日志
            	.............
 maxconn  60000  #最大连接数
 retries  3   	#3次连接失败就认为服务不可用，也可以通过后面设置
listen  集群配置
listen stats 0.0.0.0:1080   #监听端口
    stats refresh 30s   #统计页面自动刷新时间
    stats uri /stats   #统计页面url
    stats realm Haproxy Manager #进入管理解面查看状态信息
    stats auth admin:admin  #统计页面用户名和密码设置
  #stats hide-version   #隐藏统计页面上HAProxy的版本信息
listen  websrv-rewrite 0.0.0.0:80
   balance roundrobin
   server  web1 192.168.2.100:80 check inter 2000 rise 2 fall 5
   server  web2 192.168.2.200:80 check inter 2000 rise 2 fall 5
[root@haproxy ~]# systemctl restart haproxy
客户端访问http://192.168.4.5:1080/stats测试状态监控页面是否正常















